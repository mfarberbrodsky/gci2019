# Chinese Word Segmentation
This is a model for tokenizing simplified Chinese strings into words.

## Usage
Run `python tokenizer.py text`, where text is some chinese text.

Example:
```shell script
$ python3 tokenizer.py 结巴”中文分词
Tokens: ['结', '巴', '”中文分词']
```

## References
I used this project as reference (a Chinese tokenizer for js):
https://github.com/yishn/chinese-tokenizer